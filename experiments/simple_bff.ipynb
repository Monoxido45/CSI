{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"BFF toy example\"\n",
        "execute:\n",
        "  warning: false\n",
        "  keep-ipynb: true\n",
        "format: pdf\n",
        "---"
      ],
      "id": "f926b080"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# importing packages\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# font size\n",
        "sns.set_style(\"white\", rc={\"font_scale\": 1.5})\n",
        "\n",
        "# loforest and locart functions\n",
        "from CP2LFI.loforest import ConformalLoforest\n",
        "from CP2LFI.scores import LambdaScore\n",
        "\n",
        "from clover import Scores\n",
        "from clover import LocartSplit\n",
        "\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "import time"
      ],
      "id": "e8fbaa55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\section{Comparing our methods for a BFF example with normal distribution}\n",
        "Taking $\\theta \\in \\Theta = [-5,5]$ we consider a one-dimensional normal prior over $\\theta$ given by:\n",
        "\\begin{align*}\n",
        "\\theta \\sim N(0, 0.25) ,\n",
        "\\end{align*}\n",
        "with the likelihood of X given by:\n",
        "\\begin{align*}\n",
        "X \\sim N(\\theta, 1).\n",
        "\\end{align*}\n",
        "In this case, we obtain the $1 - \\alpha$ credibility region of $\\theta$ by obtaining $C_{\\theta}$ such that:\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(\\{\\theta: f(\\theta|x) \\geq C_{\\theta}\\}) = 1 - \\alpha .\n",
        "\\end{align*}\n",
        "\n",
        "Now we define the main functions to simulate all samples and compute naive quantiles:"
      ],
      "id": "8866f008"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sim_X(n, theta):\n",
        "    X = np.random.normal(theta, 1, n)\n",
        "    return X\n",
        "\n",
        "\n",
        "def sim_lambda(B, N, theta, sigma=0.25):\n",
        "    lambdas = np.zeros(B)\n",
        "    for i in range(0, B):\n",
        "        X = sim_X(N, theta)\n",
        "        lambdas[i] = compute_pdf_posterior(theta, X, sigma=sigma)\n",
        "    return lambdas\n",
        "\n",
        "\n",
        "def sample_posterior(n, N, seed=45, sigma=0.25):\n",
        "    np.random.seed(seed)\n",
        "    thetas = np.random.uniform(-5, 5, size=n)\n",
        "    lambdas = np.zeros(n)\n",
        "    i = 0\n",
        "    for theta in thetas:\n",
        "        X = sim_X(N, theta)\n",
        "        lambdas[i] = compute_pdf_posterior(theta, X, sigma=sigma)\n",
        "        i += 1\n",
        "    return thetas, lambdas\n",
        "\n",
        "\n",
        "def compute_pdf_posterior(theta, x, sigma=0.25):\n",
        "    n = x.shape[0]\n",
        "    mu_value = (1 / ((1 / sigma) + n)) * (np.sum(x))\n",
        "    sigma_value = ((1 / sigma) + n) ** (-1)\n",
        "    return -stats.norm.pdf(theta, loc=mu_value, scale=np.sqrt(sigma_value))\n",
        "\n",
        "\n",
        "# naive method\n",
        "def naive(alpha, B=1000, N=100, lower=-5, upper=5, seed=250, naive_n=100, sigma=0.25):\n",
        "    np.random.seed(seed)\n",
        "    n_grid = int(B / naive_n)\n",
        "    thetas = np.linspace(lower, upper, n_grid)\n",
        "    quantiles = {}\n",
        "    for theta in thetas:\n",
        "        lambdas = sim_lambda(B=naive_n, N=N, theta=theta, sigma=sigma)\n",
        "        quantiles[theta] = np.quantile(lambdas, q=1 - alpha)\n",
        "    return quantiles\n",
        "\n",
        "\n",
        "# naive predict function\n",
        "def predict_naive_quantile(theta_grid, quantiles_dict):\n",
        "    thetas_values = np.array(list(quantiles_dict.keys()))\n",
        "    quantiles_list = []\n",
        "    for theta in theta_grid:\n",
        "        idx = thetas_values[int(np.argmin(np.abs(theta - thetas_values)))]\n",
        "        quantiles_list.append(quantiles_dict[idx])\n",
        "    return quantiles_list"
      ],
      "id": "1d5d4f1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets also check if the statistics are not invariant:"
      ],
      "id": "73994e7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "np.random.seed(45)\n",
        "theta_vec = np.array([-4.5, -3, -1.5, 0, 1.5, 3, 4.5])\n",
        "simulations = [sim_lambda(B = 5000, N = 100, theta = theta) for theta in theta_vec]\n",
        "\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Define a list of colors\n",
        "num_columns = 4  # \n",
        "\n",
        "# Calculate the number of rows needed for the subplots\n",
        "num_rows = len(simulations) // num_columns\n",
        "num_rows += len(simulations) % num_columns\n",
        "\n",
        "# Reshape the subplot grid to accommodate all simulations\n",
        "fig, axs = plt.subplots(num_rows, num_columns, figsize=(16, 10))\n",
        "\n",
        "# Flatten the axs array for easy iteration\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, simulation in enumerate(simulations):\n",
        "    lambdas = simulation\n",
        "    axs[i].hist(lambdas, bins=30, density=True, alpha=0.6, color=colors[i % len(colors)])  # Use different color for each histogram\n",
        "    axs[i].set_title(r'$\\theta$ = {}'.format(theta_vec[i]))\n",
        "    axs[i].set_xlabel(r'$\\lambda$')\n",
        "    axs[i].set_ylabel('Frequency')\n",
        "\n",
        "# Remove unused subplots\n",
        "if len(simulations) < len(axs):\n",
        "    for i in range(len(simulations), len(axs)):\n",
        "        fig.delaxes(axs[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d31918a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we saw, the statistics appear to have different distributions as we change $\\theta$ values. Lets now compare our methods to all the others for several N:\n"
      ],
      "id": "7812279c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# function to obtain all comparing methods quantile\n",
        "def obtain_quantiles(\n",
        "    thetas,\n",
        "    N,\n",
        "    B=1000,\n",
        "    alpha=0.05,\n",
        "    naive_seed=45,\n",
        "    min_samples_leaf=100,\n",
        "    naive_n=500,\n",
        "    sigma=0.25,\n",
        "    sample_seed=25,\n",
        "):\n",
        "    # fitting and predicting naive\n",
        "    naive_quantiles = naive(\n",
        "        alpha=alpha, B=B, N=N, naive_n=naive_n, sigma=sigma, seed=naive_seed\n",
        "    )\n",
        "    naive_list = predict_naive_quantile(thetas, naive_quantiles)\n",
        "\n",
        "    # simulating to fit models\n",
        "    theta_sim, model_lambdas = sample_posterior(n=B, N=N, seed=sample_seed)\n",
        "    model_thetas = theta_sim.reshape(-1, 1)\n",
        "\n",
        "    locart_object = LocartSplit(\n",
        "        LambdaScore, None, alpha=alpha, is_fitted=True, split_calib=False\n",
        "    )\n",
        "    locart_quantiles = locart_object.calib(\n",
        "        model_thetas, model_lambdas, min_samples_leaf=min_samples_leaf\n",
        "    )\n",
        "\n",
        "    # loforest quantiles\n",
        "    loforest_object = ConformalLoforest(\n",
        "        LambdaScore, None, alpha=alpha, is_fitted=True, split_calib=False\n",
        "    )\n",
        "    loforest_object.calibrate(\n",
        "        model_thetas, model_lambdas, min_samples_leaf=min_samples_leaf\n",
        "    )\n",
        "\n",
        "    # boosting quantiles\n",
        "    model = HistGradientBoostingRegressor(\n",
        "        loss=\"quantile\",\n",
        "        max_iter=100,\n",
        "        max_depth=3,\n",
        "        quantile=1 - alpha,\n",
        "        random_state=105,\n",
        "        n_iter_no_change=15,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    model.fit(model_thetas, model_lambdas)\n",
        "\n",
        "    # naive quantiles\n",
        "    naive_list = predict_naive_quantile(thetas, naive_quantiles)\n",
        "\n",
        "    # locart quantiles\n",
        "    idxs = locart_object.cart.apply(thetas.reshape(-1, 1))\n",
        "    list_locart_quantiles = [locart_quantiles[idx] for idx in idxs]\n",
        "\n",
        "    # loforest\n",
        "    loforest_cutoffs = loforest_object.compute_cutoffs(thetas.reshape(-1, 1))\n",
        "\n",
        "    # boosting\n",
        "    boosting_quantiles = model.predict(thetas.reshape(-1, 1))\n",
        "\n",
        "    # dictionary of quantiles\n",
        "    quantile_dict = {\n",
        "        \"naive\": naive_list,\n",
        "        \"locart\": list_locart_quantiles,\n",
        "        \"loforest\": loforest_cutoffs,\n",
        "        \"boosting\": boosting_quantiles,\n",
        "    }\n",
        "\n",
        "    return quantile_dict\n",
        "\n",
        "\n",
        "# evaluate coverage for several N's and B = 1000\n",
        "def evaluate_coverage_N(\n",
        "    thetas,\n",
        "    N=np.array([10, 100, 1000]),\n",
        "    B=1000,\n",
        "    alpha=0.05,\n",
        "    n=1000,\n",
        "    seed=45,\n",
        "    min_samples_leaf=100,\n",
        "    naive_n=100,\n",
        "    sigma=0.25,\n",
        "):\n",
        "    coverage_data = np.zeros((thetas.shape[0] * N.shape[0], 4))\n",
        "    N_list = []\n",
        "    N_list_cover = []\n",
        "    methods_list = []\n",
        "    np.random.seed(seed)\n",
        "    seeds = np.random.choice(np.arange(0, 10**4, 1), N.shape[0], replace=False)\n",
        "    sample_seeds = np.random.choice(np.arange(0, 10**4, 1), N.shape[0], replace=False)\n",
        "    k = 0\n",
        "    j = 0\n",
        "    for N_fixed in tqdm(N, desc=\"Computing coverage for each N\"):\n",
        "        # computing all quantiles for fixed N\n",
        "        quantiles_dict = obtain_quantiles(\n",
        "            thetas,\n",
        "            N=N_fixed,\n",
        "            B=B,\n",
        "            alpha=alpha,\n",
        "            naive_seed=seeds[k],\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            naive_n=naive_n,\n",
        "            sample_seed=sample_seeds[k],\n",
        "            sigma=sigma,\n",
        "        )\n",
        "        err_data = np.zeros((thetas.shape[0], 4))\n",
        "        l = 0\n",
        "        for theta in tqdm(thetas, desc=\"Computing coverage for each method\"):\n",
        "            # generating several lambdas\n",
        "            lambda_stat = sim_lambda(B=n, N=N_fixed, theta=theta, sigma=sigma)\n",
        "\n",
        "            # comparing coverage of methods\n",
        "            locart_cover = np.mean(lambda_stat <= quantiles_dict[\"locart\"][l])\n",
        "            loforest_cover = np.mean(lambda_stat <= quantiles_dict[\"loforest\"][l])\n",
        "            boosting_cover = np.mean(lambda_stat <= quantiles_dict[\"boosting\"][l])\n",
        "            naive_cover = np.mean(lambda_stat <= quantiles_dict[\"naive\"][l])\n",
        "\n",
        "            # appending the errors\n",
        "            err_locart = np.abs(locart_cover - (1 - alpha))\n",
        "            err_loforest = np.abs(loforest_cover - (1 - alpha))\n",
        "            err_boosting = np.abs(boosting_cover - (1 - alpha))\n",
        "            err_naive = np.abs(naive_cover - (1 - alpha))\n",
        "\n",
        "            # saving in numpy array\n",
        "            err_data[l, :] = np.array(\n",
        "                [err_locart, err_loforest, err_boosting, err_naive]\n",
        "            )\n",
        "\n",
        "            coverage_data[j, :] = np.array(\n",
        "                [locart_cover, loforest_cover, boosting_cover, naive_cover]\n",
        "            )\n",
        "            N_list_cover.append(N_fixed)\n",
        "\n",
        "            j += 1\n",
        "            l += 1\n",
        "        methods_list.extend([\"LOCART\", \"LOFOREST\", \"boosting\", \"naive\"])\n",
        "        if k == 0:\n",
        "            mae_vector = np.mean(err_data, axis=0)\n",
        "            std_vector = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
        "        else:\n",
        "            mean = np.mean(err_data, axis=0)\n",
        "            std = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
        "            mae_vector, std_vector = np.concatenate((mae_vector, mean)), np.concatenate(\n",
        "                (std_vector, std)\n",
        "            )\n",
        "        k += 1\n",
        "        N_list.extend([N_fixed] * 4)\n",
        "\n",
        "    # obtaining MAE and standard error for each method\n",
        "    stats_data = pd.DataFrame(\n",
        "        {\n",
        "            \"methods\": methods_list,\n",
        "            \"N\": N_list,\n",
        "            \"MAE\": mae_vector,\n",
        "            \"se\": std_vector,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    coverage_data = pd.DataFrame(\n",
        "        {\n",
        "            \"thetas\": np.tile(thetas, N.shape[0]),\n",
        "            \"N\": N_list_cover,\n",
        "            \"LOCART\": coverage_data[:, 0],\n",
        "            \"LOFOREST\": coverage_data[:, 1],\n",
        "            \"boosting\": coverage_data[:, 2],\n",
        "            \"naive\": coverage_data[:, 3],\n",
        "        }\n",
        "    )\n",
        "    return [stats_data, coverage_data]"
      ],
      "id": "735d60bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing for $B = 5000$:"
      ],
      "id": "861050e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | output: false\n",
        "n_out = 750\n",
        "thetas = np.linspace(-4.999, 4.999, n_out)\n",
        "\n",
        "coverage_data_N_5000 = evaluate_coverage_N(\n",
        "    thetas,\n",
        "    N=np.array([10, 100, 1000, 2000]),\n",
        "    B=5000,\n",
        "    naive_n=500,\n",
        "    min_samples_leaf=300,\n",
        "    n=2000,\n",
        "    seed=1250,\n",
        ")"
      ],
      "id": "e651c6a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting all results:"
      ],
      "id": "278db66b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(16, 8))\n",
        "# data for first subplot\n",
        "data_10 = coverage_data_N_5000[0].query(\"N == 10\")\n",
        "keys = data_10.iloc[:, 0].values\n",
        "mae = data_10.iloc[:, 2].values\n",
        "std_err = data_10.iloc[:, 3].values\n",
        "ax[0].errorbar(keys, mae, yerr=std_err, fmt=\"o\")\n",
        "ax[0].set_xlabel(\"Method\")\n",
        "ax[0].set_ylabel(\"Error\")\n",
        "ax[0].set_title(\"Estimated MAE for N = 10\")\n",
        "\n",
        "\n",
        "data_100 = coverage_data_N_5000[0].query(\"N == 100\")\n",
        "keys = data_100.iloc[:, 0].values\n",
        "mae = data_100.iloc[:, 2].values\n",
        "std_err = data_100.iloc[:, 3].values\n",
        "ax[1].errorbar(keys, mae, yerr=std_err, fmt=\"o\")\n",
        "ax[1].set_xlabel(\"Method\")\n",
        "ax[1].set_ylabel(\"Error\")\n",
        "ax[1].set_title(\"Estimated MAE for N = 100\")\n",
        "\n",
        "\n",
        "data_1000 = coverage_data_N_5000[0].query(\"N == 1000\")\n",
        "keys = data_1000.iloc[:, 0].values\n",
        "mae = data_1000.iloc[:, 2].values\n",
        "std_err = data_1000.iloc[:, 3].values\n",
        "ax[2].errorbar(keys, mae, yerr=std_err, fmt=\"o\")\n",
        "ax[2].set_xlabel(\"Method\")\n",
        "ax[2].set_ylabel(\"Error\")\n",
        "ax[2].set_title(\"Estimated MAE for N = 1000\")\n",
        "\n",
        "data_2000 = coverage_data_N_5000[0].query(\"N == 2000\")\n",
        "keys = data_2000.iloc[:, 0].values\n",
        "mae = data_2000.iloc[:, 2].values\n",
        "std_err = data_2000.iloc[:, 3].values\n",
        "ax[3].errorbar(keys, mae, yerr=std_err, fmt=\"o\")\n",
        "ax[3].set_xlabel(\"Method\")\n",
        "ax[3].set_ylabel(\"Error\")\n",
        "ax[3].set_title(\"Estimated MAE for N = 2000\")\n",
        "\n",
        "for a in ax:\n",
        "    a.set_xticklabels(a.get_xticklabels(), rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "9e622a85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "# plotting coverage\n",
        "coverage_data_melted = pd.melt(\n",
        "    coverage_data_N_5000[1],\n",
        "    id_vars=[\"thetas\", \"N\"],\n",
        "    var_name=\"method\",\n",
        "    value_name=\"coverage\",\n",
        ")\n",
        "# Plot the coverage as function of the thetas generated\n",
        "sns.set(style=\"white\", font_scale=3.5)\n",
        "g = sns.FacetGrid(\n",
        "    coverage_data_melted,\n",
        "    row=\"N\",\n",
        "    col=\"method\",\n",
        "    hue=\"method\",\n",
        "    height=8,\n",
        "    aspect=1.60,\n",
        "    palette=\"Set1\",\n",
        "    margin_titles=True,\n",
        ")\n",
        "g.map(sns.lineplot, \"thetas\", \"coverage\")\n",
        "g.add_legend(bbox_to_anchor=(1., 0.5), loc=2, borderaxespad=0.)\n",
        "g.set_titles(\"{col_name}\")\n",
        "g.set_xlabels(r\"$\\theta$\")\n",
        "g.set_ylabels(\"Coverage\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "8d2d7773",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets evaluate the performance of each method as a function of the sample size $B$ for several $N$, in this case $N \\in \\{1, 50, 100, 1000\\}$ and $B \\in \\{500, 1000, 5000, 10000\\}$:"
      ],
      "id": "ef71a98a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# function to compute MAE stats for several B's and N's\n",
        "def compute_MAE_N(\n",
        "    thetas,\n",
        "    N=np.array([1, 10, 100, 1000]),\n",
        "    B=np.array([500, 1000, 5000, 10000, 15000, 20000]),\n",
        "    alpha=0.05,\n",
        "    n=1000,\n",
        "    seed=45,\n",
        "    min_samples_leaf=100,\n",
        "    naive_n=100,\n",
        "    sigma=0.25,\n",
        "):\n",
        "    coverage_data = np.zeros((thetas.shape[0] * N.shape[0], 4))\n",
        "    N_list = []\n",
        "    N_list_cover = []\n",
        "    methods_list = []\n",
        "    B_list = []\n",
        "    j = 0\n",
        "    np.random.seed(seed)\n",
        "    for N_fixed in tqdm(N, desc=\"Computing coverage for each N\"):\n",
        "        seeds = np.random.choice(\n",
        "            np.arange(0, 10**4, 1),\n",
        "            B.shape[0],\n",
        "            replace=False,\n",
        "        )\n",
        "        sample_seeds = np.random.choice(\n",
        "            np.arange(0, 10**4, 1),\n",
        "            B.shape[0],\n",
        "            replace=False,\n",
        "        )\n",
        "        k = 0\n",
        "        for B_fixed in B:\n",
        "            # computing all quantiles for fixed N\n",
        "            quantiles_dict = obtain_quantiles(\n",
        "                thetas,\n",
        "                N=N_fixed,\n",
        "                B=B_fixed,\n",
        "                alpha=alpha,\n",
        "                naive_seed=seeds[k],\n",
        "                min_samples_leaf=min_samples_leaf,\n",
        "                naive_n=naive_n,\n",
        "                sample_seed=sample_seeds[k],\n",
        "                sigma=sigma,\n",
        "            )\n",
        "            err_data = np.zeros((thetas.shape[0], 4))\n",
        "            l = 0\n",
        "            for theta in thetas:\n",
        "                # generating several lambdas\n",
        "                lambda_stat = sim_lambda(\n",
        "                    B=n,\n",
        "                    N=N_fixed,\n",
        "                    theta=theta,\n",
        "                    sigma=sigma,\n",
        "                )\n",
        "\n",
        "                # comparing coverage of methods\n",
        "                locart_cover = np.mean(lambda_stat <= quantiles_dict[\"locart\"][l])\n",
        "                loforest_cover = np.mean(lambda_stat <= quantiles_dict[\"loforest\"][l])\n",
        "                boosting_cover = np.mean(lambda_stat <= quantiles_dict[\"boosting\"][l])\n",
        "                naive_cover = np.mean(lambda_stat <= quantiles_dict[\"naive\"][l])\n",
        "\n",
        "                # appending the errors\n",
        "                err_locart = np.abs(locart_cover - (1 - alpha))\n",
        "                err_loforest = np.abs(loforest_cover - (1 - alpha))\n",
        "                err_boosting = np.abs(boosting_cover - (1 - alpha))\n",
        "                err_naive = np.abs(naive_cover - (1 - alpha))\n",
        "\n",
        "                # saving in numpy array\n",
        "                err_data[l, :] = np.array(\n",
        "                    [err_locart, err_loforest, err_boosting, err_naive]\n",
        "                )\n",
        "\n",
        "                l += 1\n",
        "            methods_list.extend([\"LOCART\", \"LOFOREST\", \"boosting\", \"naive\"])\n",
        "            N_list.extend([N_fixed] * 4)\n",
        "            B_list.extend([B_fixed] * 4)\n",
        "            if j == 0:\n",
        "                mae_vector = np.mean(err_data, axis=0)\n",
        "                std_vector = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
        "            else:\n",
        "                mean = np.mean(err_data, axis=0)\n",
        "                std = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
        "                mae_vector, std_vector = np.concatenate(\n",
        "                    (mae_vector, mean)\n",
        "                ), np.concatenate((std_vector, std))\n",
        "            k += 1\n",
        "            j += 1\n",
        "\n",
        "    # obtaining MAE and standard error for each method\n",
        "    stats_data = pd.DataFrame(\n",
        "        {\n",
        "            \"methods\": methods_list,\n",
        "            \"N\": N_list,\n",
        "            \"B\": B_list,\n",
        "            \"MAE\": mae_vector,\n",
        "            \"se\": std_vector,\n",
        "        }\n",
        "    )\n",
        "    return stats_data"
      ],
      "id": "467cc13f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | output: false\n",
        "n_out = 500\n",
        "thetas = np.linspace(-4.999, 4.999, n_out)\n",
        "mae_stats = compute_MAE_N(\n",
        "    thetas,\n",
        "    N=np.array([1, 50, 100, 1000]),\n",
        "    naive_n=500,\n",
        "    min_samples_leaf=300,\n",
        "    seed=1250,\n",
        ")"
      ],
      "id": "6438422f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting all results:"
      ],
      "id": "753ef358"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "# Create a line plot with error bars\n",
        "sns.set(style=\"white\", font_scale=3.5)\n",
        "g = sns.FacetGrid(\n",
        "    mae_stats,\n",
        "    col=\"N\",\n",
        "    col_wrap=2,\n",
        "    height=8,\n",
        "    aspect=1.60,\n",
        "    hue=\"methods\",\n",
        "    palette=\"Set1\",\n",
        "    margin_titles=True,\n",
        ")\n",
        "g.map(plt.errorbar, \"B\", \"MAE\", \"se\", marker=\"o\", fmt=\"-o\")\n",
        "g.add_legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "c45d9684",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}