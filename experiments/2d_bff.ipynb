{
 "cells": [
  {
   "cell_type": "raw",
   "id": "01ddf6ad",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"2 parameter BFF toy example\"\n",
    "execute:\n",
    "  warning: false\n",
    "  keep-ipynb: true\n",
    "format: pdf\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c88a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# font size\n",
    "sns.set_style(\"white\", rc={\"font_scale\": 1.5})\n",
    "\n",
    "# loforest and locart functions\n",
    "from CP2LFI.loforest import ConformalLoforest\n",
    "from CP2LFI.scores import LambdaScore\n",
    "\n",
    "from clover import Scores\n",
    "from clover import LocartSplit\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482b5e",
   "metadata": {},
   "source": [
    "\\section{Comparing our methods for a BFF example with normal distribution}\n",
    "Taking $(\\mu, \\sigma^2) \\in \\Theta = [-5,5] \\times (0, 1]$ we consider a normal-inverse gamma prior over $(\\mu, \\sigma^2)$ given by:\n",
    "\\begin{align*}\n",
    "\\mu|\\sigma^2 \\sim N(0, 0.25 \\sigma^2) , \\\\\n",
    "\\sigma^2 \\sim IG(2 ,1) ,\n",
    "\\end{align*}\n",
    "with the likelihood of $X$ given by:\n",
    "\\begin{align*}\n",
    "X|\\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2).\n",
    "\\end{align*}\n",
    "In this case, we obtain the $1 - \\alpha$ credibility region of $\\theta$ by obtaining $C_{\\theta}$ such that:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(\\{(\\mu, \\sigma^2): f(\\mu, \\sigma^2|x) \\geq C_{\\left( \\mu, \\sigma^2 \\right)}\\}) = 1 - \\alpha .\n",
    "\\end{align*}\n",
    "\n",
    "Now we define the main functions to simulate all samples and compute naive quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f578b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_X(n, mu, sigma_2):\n",
    "    X = np.random.normal(mu, np.sqrt(sigma_2), n)\n",
    "    return X\n",
    "\n",
    "\n",
    "def sim_lambda(B, N, mu, sigma_2, l=4, a=2, beta=1):\n",
    "    lambdas = np.zeros(B)\n",
    "    for i in range(0, B):\n",
    "        X = sim_X(N, mu, sigma_2)\n",
    "        lambdas[i] = compute_pdf_posterior(\n",
    "            mu,\n",
    "            sigma_2,\n",
    "            X,\n",
    "            l=l,\n",
    "            a=a,\n",
    "            beta=beta,\n",
    "        )\n",
    "    return lambdas\n",
    "\n",
    "\n",
    "def sample_posterior(B, N, seed=45, l=4, a=2, beta=1):\n",
    "    np.random.seed(seed)\n",
    "    thetas = np.c_[np.random.uniform(-5, 5, B), np.random.uniform(0, 1, B)]\n",
    "    lambdas = np.zeros(n)\n",
    "    i = 0\n",
    "    for mu, sigma_2 in thetas:\n",
    "        X = sim_X(N, mu, sigma_2)\n",
    "        lambdas[i] = compute_pdf_posterior(\n",
    "            mu,\n",
    "            sigma_2,\n",
    "            X,\n",
    "            l=l,\n",
    "            a=a,\n",
    "            beta=beta,\n",
    "        )\n",
    "        i += 1\n",
    "    return thetas, lambdas\n",
    "\n",
    "\n",
    "def compute_pdf_posterior(mu, sigma_2, x, l=4, a=2, beta=1):\n",
    "    n = x.shape[0]\n",
    "    sum_squares = n * np.var(x)\n",
    "    x_bar = np.mean(x)\n",
    "\n",
    "    # posterior parameters\n",
    "    mu_value = (1 / (l + n)) * (np.sum(x))\n",
    "    l_value = l + n\n",
    "    alpha_value = a + (n / 2)\n",
    "    beta_value = (\n",
    "        beta + ((1 / 2) * (sum_squares)) + ((n * l) / (l + n) * ((x_bar**2) / 2))\n",
    "    )\n",
    "\n",
    "    # pdf for mu\n",
    "    f_mu = stats.norm.pdf(mu, loc=mu_value, scale=np.sqrt(sigma_2 / l_value))\n",
    "\n",
    "    f_sigma_2 = stats.invgamma.pdf(sigma_2, a=alpha_value, scale=beta_value)\n",
    "\n",
    "    return -(f_mu * f_sigma_2)\n",
    "\n",
    "\n",
    "# naive method\n",
    "def naive(\n",
    "    alpha,\n",
    "    B=1000,\n",
    "    N=100,\n",
    "    lower=-5,\n",
    "    upper=5,\n",
    "    seed=250,\n",
    "    naive_n=100,\n",
    "    l=4,\n",
    "    a=2,\n",
    "    beta=1,\n",
    "):\n",
    "    n_grid = int(B / naive_n)\n",
    "    mu = np.linspace(-5, 5, n_grid)\n",
    "    sigma_2 = np.linspace(0.0001, 1, n_grid)\n",
    "    quantiles = {}\n",
    "    for mu, sigma_2 in itertools.product(mu, sigma_2):\n",
    "        lambdas = sim_lambda(\n",
    "            B=int(np.sqrt(naive_n)),\n",
    "            N=N,\n",
    "            mu=mu,\n",
    "            sigma_2=sigma_2,\n",
    "            l=l,\n",
    "            a=a,\n",
    "            beta=beta,\n",
    "        )\n",
    "        quantiles[(mu, sigma_2)] = np.quantile(lambdas, q=1 - alpha)\n",
    "    return quantiles\n",
    "\n",
    "\n",
    "# naive predict function\n",
    "def predict_naive_quantile(theta_grid, quantiles_dict):\n",
    "    thetas_values = np.array(list(quantiles_dict.keys()))\n",
    "    quantiles_list = []\n",
    "    for x in theta_grid:\n",
    "        distances = np.linalg.norm(thetas_values - x, axis=1)\n",
    "        idx = thetas_values[np.argmin(distances)]\n",
    "        quantiles_list.append(quantiles_dict[tuple(idx)])\n",
    "    return quantiles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196ad5a",
   "metadata": {},
   "source": [
    "Lets now compare our methods to all the others for several N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f01f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_quantiles(\n",
    "    thetas,\n",
    "    N,\n",
    "    B=1000,\n",
    "    alpha=0.05,\n",
    "    naive_seed=45,\n",
    "    min_samples_leaf=100,\n",
    "    naive_n=500,\n",
    "    sample_seed=25,\n",
    "    l=4,\n",
    "    a=2,\n",
    "    beta=1,\n",
    "):\n",
    "    # fitting and predicting naive\n",
    "    naive_quantiles = naive(\n",
    "        alpha=alpha,\n",
    "        B=B,\n",
    "        N=N,\n",
    "        naive_n=naive_n,\n",
    "        sigma=sigma,\n",
    "        seed=naive_seed,\n",
    "        l=l,\n",
    "        a=a,\n",
    "        beta=beta,\n",
    "    )\n",
    "\n",
    "    # simulating to fit models\n",
    "    model_thetas, model_lambdas = sample_posterior(n=B, N=N, seed=sample_seed)\n",
    "\n",
    "    locart_object = LocartSplit(\n",
    "        LambdaScore, None, alpha=alpha, is_fitted=True, split_calib=False\n",
    "    )\n",
    "    locart_quantiles = locart_object.calib(\n",
    "        model_thetas, model_lambdas, min_samples_leaf=min_samples_leaf\n",
    "    )\n",
    "\n",
    "    # loforest quantiles\n",
    "    loforest_object = ConformalLoforest(\n",
    "        LambdaScore, None, alpha=alpha, is_fitted=True, split_calib=False\n",
    "    )\n",
    "    loforest_object.calibrate(\n",
    "        model_thetas, model_lambdas, min_samples_leaf=min_samples_leaf\n",
    "    )\n",
    "\n",
    "    # boosting quantiles\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        max_iter=100,\n",
    "        max_depth=3,\n",
    "        quantile=1 - alpha,\n",
    "        random_state=105,\n",
    "        n_iter_no_change=15,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    model.fit(model_thetas, model_lambdas)\n",
    "\n",
    "    # naive quantiles\n",
    "    naive_list = predict_naive_quantile(thetas, naive_quantiles)\n",
    "\n",
    "    # locart quantiles\n",
    "    idxs = locart_object.cart.apply(thetas.reshape(-1, 1))\n",
    "    list_locart_quantiles = [locart_quantiles[idx] for idx in idxs]\n",
    "\n",
    "    # loforest\n",
    "    loforest_cutoffs = loforest_object.compute_cutoffs(thetas.reshape(-1, 1))\n",
    "\n",
    "    # boosting\n",
    "    boosting_quantiles = model.predict(thetas.reshape(-1, 1))\n",
    "\n",
    "    # dictionary of quantiles\n",
    "    quantile_dict = {\n",
    "        \"naive\": naive_list,\n",
    "        \"locart\": list_locart_quantiles,\n",
    "        \"loforest\": loforest_cutoffs,\n",
    "        \"boosting\": boosting_quantiles,\n",
    "    }\n",
    "\n",
    "    return quantile_dict\n",
    "\n",
    "\n",
    "# evaluate coverage for several N's and B = 1000\n",
    "def evaluate_coverage_N(\n",
    "    thetas,\n",
    "    N=np.array([10, 100, 1000]),\n",
    "    B=1000,\n",
    "    alpha=0.05,\n",
    "    n=1000,\n",
    "    seed=45,\n",
    "    min_samples_leaf=100,\n",
    "    naive_n=100,\n",
    "    l=4,\n",
    "    a=2,\n",
    "    beta=1,\n",
    "):\n",
    "    coverage_data = np.zeros((thetas.shape[0] * N.shape[0], 4))\n",
    "    N_list = []\n",
    "    N_list_cover = []\n",
    "    methods_list = []\n",
    "    np.random.seed(seed)\n",
    "    seeds = np.random.choice(\n",
    "        np.arange(0, 10**4, 1),\n",
    "        N.shape[0],\n",
    "        replace=False,\n",
    "    )\n",
    "    sample_seeds = np.random.choice(\n",
    "        np.arange(0, 10**4, 1),\n",
    "        N.shape[0],\n",
    "        replace=False,\n",
    "    )\n",
    "    k = 0\n",
    "    j = 0\n",
    "    for N_fixed in tqdm(N, desc=\"Computing coverage for each N\"):\n",
    "        # computing all quantiles for fixed N\n",
    "        quantiles_dict = obtain_quantiles(\n",
    "            thetas,\n",
    "            N=N_fixed,\n",
    "            B=B,\n",
    "            alpha=alpha,\n",
    "            naive_seed=seeds[k],\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            naive_n=naive_n,\n",
    "            sample_seed=sample_seeds[k],\n",
    "            l=l,\n",
    "            a=a,\n",
    "            beta=beta,\n",
    "        )\n",
    "        err_data = np.zeros((thetas.shape[0], 4))\n",
    "        l = 0\n",
    "        for theta in tqdm(thetas, desc=\"Computing coverage for each method\"):\n",
    "            # generating several lambdas\n",
    "            lambda_stat = sim_lambda(B=n, N=N_fixed, theta=theta, sigma=sigma)\n",
    "\n",
    "            # comparing coverage of methods\n",
    "            locart_cover = np.mean(lambda_stat <= quantiles_dict[\"locart\"][l])\n",
    "            loforest_cover = np.mean(lambda_stat <= quantiles_dict[\"loforest\"][l])\n",
    "            boosting_cover = np.mean(lambda_stat <= quantiles_dict[\"boosting\"][l])\n",
    "            naive_cover = np.mean(lambda_stat <= quantiles_dict[\"naive\"][l])\n",
    "\n",
    "            # appending the errors\n",
    "            err_locart = np.abs(locart_cover - (1 - alpha))\n",
    "            err_loforest = np.abs(loforest_cover - (1 - alpha))\n",
    "            err_boosting = np.abs(boosting_cover - (1 - alpha))\n",
    "            err_naive = np.abs(naive_cover - (1 - alpha))\n",
    "\n",
    "            # saving in numpy array\n",
    "            err_data[l, :] = np.array(\n",
    "                [err_locart, err_loforest, err_boosting, err_naive]\n",
    "            )\n",
    "            N_list_cover.append(N_fixed)\n",
    "\n",
    "            j += 1\n",
    "            l += 1\n",
    "        methods_list.extend([\"LOCART\", \"LOFOREST\", \"boosting\", \"naive\"])\n",
    "        if k == 0:\n",
    "            mae_vector = np.mean(err_data, axis=0)\n",
    "            std_vector = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
    "        else:\n",
    "            mean = np.mean(err_data, axis=0)\n",
    "            std = np.std(err_data, axis=0) / (np.sqrt(thetas.shape[0]))\n",
    "            mae_vector, std_vector = np.concatenate((mae_vector, mean)), np.concatenate(\n",
    "                (std_vector, std)\n",
    "            )\n",
    "        k += 1\n",
    "        N_list.extend([N_fixed] * 4)\n",
    "\n",
    "    # obtaining MAE and standard error for each method\n",
    "    stats_data = pd.DataFrame(\n",
    "        {\n",
    "            \"methods\": methods_list,\n",
    "            \"N\": N_list,\n",
    "            \"MAE\": mae_vector,\n",
    "            \"se\": std_vector,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    coverage_data = pd.DataFrame(\n",
    "        {\n",
    "            \"thetas\": np.tile(thetas, N.shape[0]),\n",
    "            \"N\": N_list_cover,\n",
    "            \"LOCART\": coverage_data[:, 0],\n",
    "            \"LOFOREST\": coverage_data[:, 1],\n",
    "            \"boosting\": coverage_data[:, 2],\n",
    "            \"naive\": coverage_data[:, 3],\n",
    "        }\n",
    "    )\n",
    "    return [stats_data, coverage_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab757b",
   "metadata": {},
   "source": [
    "Testing for $B = 5000$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774a817c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_out = 50\n",
    "mu_s = np.linspace(-5, 5, n_out)\n",
    "sigmas = np.linspace(0.0001, 1, n_out)\n",
    "thetas_grid = np.c_[list(itertools.product(mu_s, sigmas))]\n",
    "# printing grid size\n",
    "thetas_grid.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
